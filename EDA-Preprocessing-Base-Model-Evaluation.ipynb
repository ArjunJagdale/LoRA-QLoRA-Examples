{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary style=\"display:list-item; font-size:16px; color:blue;\">Jupyter Help</summary>\n",
    "    \n",
    "Having trouble testing your work? Double-check that you have followed the steps below to write, run, save, and test your code!\n",
    "    \n",
    "[Click here for a walkthrough GIF of the steps below](https://static-assets.codecademy.com/Courses/ds-python/jupyter-help.gif)\n",
    "\n",
    "Run all initial cells to import libraries and datasets. Then follow these steps for each question:\n",
    "    \n",
    "1. Add your solution to the cell with `## YOUR SOLUTION HERE ## `.\n",
    "2. Run the cell by selecting the `Run` button or the `Shift`+`Enter` keys.\n",
    "3. Save your work by selecting the `Save` button, the `command`+`s` keys (Mac), or `control`+`s` keys (Windows).\n",
    "4. Select the `Test Work` button at the bottom left to test your work.\n",
    "\n",
    "![Screenshot of the buttons at the top of a Jupyter Notebook. The Run and Save buttons are highlighted](https://static-assets.codecademy.com/Paths/ds-python/jupyter-buttons.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setup**\n",
    "\n",
    "We've preloaded a NumPy array below. The attribute `dtype` gives us the array's data type. The attribute `itemsize` gives us the size of an item in the array in bytes. Run the cell below to examine the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "setup"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type:  float32\n",
      "Bytes needed per element:  4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "numbers = np.array([1.23456, 2.3456, 3.456, 7.56, 15.6745], dtype = 'float32')\n",
    "print(\"Data type: \", numbers.dtype)\n",
    "print(\"Bytes needed per element: \", numbers.itemsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint (1/3): Changing Data Types in NumPy**\n",
    "\n",
    "We've written some code below to convert `numbers` to an array of FP16 numbers using the type `np.float16` function. Run the following cell to examine how the numbers change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.234  2.346  3.455  7.56  15.67 ]\n",
      "Bytes needed per element:  2\n"
     ]
    }
   ],
   "source": [
    "f16_numbers = np.float16(numbers)\n",
    "f16_bytes_per_element = f16_numbers.itemsize\n",
    "print(f16_numbers)\n",
    "print(\"Bytes needed per element: \", f16_bytes_per_element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Note that the precision has already gone down!) Now, similar to the above, you can use the `int8` type in NumPy, `np.int8`, as a function to convert its element to the signed integer datatype. \n",
    "\n",
    "Apply `np.int8` function to `numbers` and store the result as an array `int8_numbers`. Create a variable `int8_bytes_per_element` and set it to the bytes needed per element in this new array. Print both values to examine the output. \n",
    "\n",
    "**Don't forget to run the cell and save the notebook before selecting `Test Work`! Open the `Jupyter Help` toggle at the top of the notebook for more details.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "cp1"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  2  3  7 15]\n",
      "Bytes needed per element:  1\n"
     ]
    }
   ],
   "source": [
    "### YOUR SOLUTION HERE ###\n",
    "int8_numbers = np.int8(numbers)\n",
    "int8_bytes_per_element = int8_numbers.itemsize\n",
    "\n",
    "print(int8_numbers)\n",
    "print(\"Bytes needed per element: \", int8_bytes_per_element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint (2/3): Quantization in NumPy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will assume that the original array `numbers` in the previous checkpoint represent weights in a deep-learning model and test a toy function to quantize it.\n",
    "\n",
    "While the above method converted the values in the numbers array to integers, it could be better. It merely rounded down the numbers to get an integer value and lost some information in the original array. For instance, if all the weights were numerical values `< 1`, `np.int8` would round them down to `0`! To do justice to the original array, we'd need to account for its range of values relative to the range of numbers that the data type can represent.\n",
    "\n",
    "We've written a simple function in the cell below to quantize arrays using a type of quantization known as \"Absmax quantization\". Run this cell to define the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "function"
    ]
   },
   "outputs": [],
   "source": [
    "def quantize(x):\n",
    "    # scaling the values\n",
    "    scale = 127 / max(abs(x))\n",
    "    # quantizing\n",
    "    quantized_value = (scale * x).round()\n",
    "    return quantized_value.astype(np.int8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply this function to the array `numbers` and store the result as `quantized_array`. Print the array and its `itemsize` to examine the result.\n",
    "\n",
    "**Don't forget to run the cell and save the notebook before selecting `Test Work`! Open the `Jupyter Help` toggle at the top of the notebook for more details.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "cp2"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 10  19  28  61 127]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "## YOUR SOLUTION HERE ##\n",
    "#quantized_array \n",
    "quantized_array = quantize(numbers)\n",
    "print(quantized_array)\n",
    "print(quantized_array.itemsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint (3/3): Quantization in Hugging Face**\n",
    "\n",
    "We've successfully quantized an array from a `float32` array that requires 4 bytes of memory per element to an `int8` array that requires only a single byte per element! What would it mean to do the same for a large transformer model with huge weight matrices? How much would the computational memory of the model decrease?\n",
    "\n",
    "Luckily, we can answer these questions very easily with Hugging Face's `bitsandbytes` library. The documentation for this package is available at the following link: [https://huggingface.co/docs/bitsandbytes/main/en/index](https://huggingface.co/docs/bitsandbytes/main/en/index). We've already written some code below that calculates the memory footprint of `distilgpt2`, a lighter version of the GPT-2 model. Run the following cell to view the model size in bytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size in bytes:  333941784\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "torch.manual_seed(19)\n",
    "\n",
    "model_id = 'distilgpt2'\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "print(\"Model size in bytes: \", model.get_memory_footprint())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add two more arguments to the `from_pretrained` method to get the 8-bit quantized version of `distilgpt2` as follows:\n",
    "- Set `device_map` to `'auto'`\n",
    "- Set `load_in_8bit` to `True`\n",
    "\n",
    "Uncomment the line that prints the model size and run the cell.\n",
    "\n",
    "**Don't forget to run the cell and save the notebook before selecting `Test Work`! Open the `Jupyter Help` toggle at the top of the notebook for more details.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": [
     "cp3"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8-bit Quantized model size in bytes:  127649292\n"
     ]
    }
   ],
   "source": [
    "int8_quantized_model = AutoModelForCausalLM.from_pretrained(model_id, device_map= 'auto', load_in_8bit = True)\n",
    "print(\"8-bit Quantized model size in bytes: \", int8_quantized_model.get_memory_footprint())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
