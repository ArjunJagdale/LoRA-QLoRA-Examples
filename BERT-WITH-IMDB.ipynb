{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8246b47c",
   "metadata": {},
   "source": [
    "<details><summary style=\"display:list-item; font-size:16px; color:blue;\">Jupyter Help</summary>\n",
    "    \n",
    "Having trouble testing your work? Double-check that you have followed the steps below to write, run, save, and test your code!\n",
    "    \n",
    "[Click here for a walkthrough GIF of the steps below](https://static-assets.codecademy.com/Courses/ds-python/jupyter-help.gif)\n",
    "\n",
    "Run all initial cells to import libraries and datasets. Then follow these steps for each question:\n",
    "    \n",
    "1. Add your solution to the cell with `## YOUR SOLUTION HERE ## `.\n",
    "2. Run the cell by selecting the `Run` button or the `Shift`+`Enter` keys.\n",
    "3. Save your work by selecting the `Save` button, the `command`+`s` keys (Mac), or `control`+`s` keys (Windows).\n",
    "4. Select the `Test Work` button at the bottom left to test your work.\n",
    "\n",
    "![Screenshot of the buttons at the top of a Jupyter Notebook. The Run and Save buttons are highlighted](https://static-assets.codecademy.com/Paths/ds-python/jupyter-buttons.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae023367",
   "metadata": {},
   "source": [
    "**Setup**\n",
    "\n",
    "Below we import the libraries we'll use for our finetune. Take a look at the specific modules we're importing from `transformers`--they'll do the heavy lifting for us on our finetuning run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9378e93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TrainingArguments, AutoModelForSequenceClassification, Trainer\n",
    "from datasets import  Dataset\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079e0e0c",
   "metadata": {},
   "source": [
    "#### Checkpoint 1/4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8c74d0",
   "metadata": {},
   "source": [
    "In the cell below, write a line that assigns \"cuda\" to `torch.device` if \"cuda\" is available, else assigns device \"cpu\".\n",
    "\n",
    "Next, convert the imported DataFrame into a Hugging Face Dataset with the `Dataset.from_pandas()` method we covered earlier.\n",
    "\n",
    "After that conversion, we split it into training and test sets using the 'dataset' column in the original data. \n",
    "\n",
    "Don't forget to run the cell and save the notebook before selecting `Test Work`! Open the `Jupyter Help` toggle at the top of the notebook for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b318baec",
   "metadata": {
    "deletable": false,
    "scrolled": true,
    "tags": [
     "cp1"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 2500/2500 [00:00<00:00, 185720.16 examples/s]\n",
      "Filter: 100%|██████████| 2500/2500 [00:00<00:00, 206942.17 examples/s]\n"
     ]
    }
   ],
   "source": [
    "## YOUR SOLUTION HERE ##\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "data = pd.read_csv('imdb_data.csv')\n",
    "## YOUR SOLUTION HERE ##\n",
    "full_dataset = Dataset.from_pandas(data)\n",
    "\n",
    "training_set = full_dataset.filter(lambda example: example['dataset'] == 'train')\n",
    "test_set = full_dataset.filter(lambda example: example['dataset'] == 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a172e5e",
   "metadata": {},
   "source": [
    "#### Checkpoint 2/4\n",
    "\n",
    "Now, in the `tokenize_function`, tokenize the examples' `text` column, set the padding to the longest sequence in the batch and enable truncation to ensure all sequences are of the same length.\n",
    " \n",
    "Then, map the `training_set` and `test_set` to the `tokenize_function` with `batched` set to True. This will apply the tokenization in batches which is more efficient.\n",
    "\n",
    "Don't forget to run the cell and save the notebook before selecting `Test Work`! Open the `Jupyter Help` toggle at the top of the notebook for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e73ed471",
   "metadata": {
    "deletable": false,
    "scrolled": true,
    "tags": [
     "cp2"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading tokenizer_config.json: 100%|██████████| 48.0/48.0 [00:00<00:00, 301kB/s]\n",
      "Downloading config.json: 100%|██████████| 483/483 [00:00<00:00, 3.12MB/s]\n",
      "Downloading vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 62.2MB/s]\n",
      "Downloading tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 30.7MB/s]\n",
      "Map: 100%|██████████| 1250/1250 [00:00<00:00, 2589.73 examples/s]\n",
      "Map: 100%|██████████| 1250/1250 [00:00<00:00, 2932.38 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\") # this tokenizer will work for our smaller model\n",
    "## YOUR SOLUTION HERE ##\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"longest\", truncation=True)\n",
    "\n",
    "tokenized_training_set = training_set.map(tokenize_function, batched=True)\n",
    "tokenized_test_set = test_set.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d481799",
   "metadata": {},
   "source": [
    "#### Checkpoint 3/4\n",
    "\n",
    "Remember the TrainingArguments we defined when we evaluated the base model? Well, now we'll dive a little deeper.\n",
    "\n",
    "We've filled out many of these Training Arguments to sensible settings. \n",
    "\n",
    "- `output_dir` is where the model's output is saved. Don't change this, as we need to save results in `temp_results`. As the name suggests, these are wiped frequently, so don't expect to save anything there.\n",
    "- `warmup_steps` specifies the length of the warm up phase at the start of training. Gradually increasing the learning rate at the start of training can help the model avoid bad outcomes early in the training process.\n",
    "- `weight_decay` helps prevent overfitting by reducing the magnitude of the model's weights.\n",
    "- `logging_dir` specifies where to save the training logs\n",
    "- `learning_rate`, as you should know already, refers to the size of the steps the optimizer takes for each iteration of gradient descent\n",
    "-  `save_strategy` specifies how we wish to save checkpoints of the model across different epochs. Don't change this value.\n",
    "\n",
    "Now, you need to specify the number of training epochs and the batch size for both training and evaluation.\n",
    "Finetuning requires fewer epochs than pretraining.\n",
    "\n",
    "Set `num_train_epochs` to 3, `per_device_train_batch_size` to 12, and `per_device_eval_batch_size` also to 12.\n",
    "\n",
    "\n",
    "Don't forget to run the cell and save the notebook before selecting `Test Work`! Open the `Jupyter Help` toggle at the top of the notebook for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ddf801b",
   "metadata": {
    "deletable": false,
    "scrolled": true,
    "tags": [
     "cp3"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading config.json: 100%|██████████| 285/285 [00:00<00:00, 1.87MB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 17.8M/17.8M [00:00<00:00, 289MB/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./temp_results\", # do not change this\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    learning_rate=1e-4,\n",
    "    save_strategy=\"no\", # do not change this\n",
    "    ## YOUR SOLUTION HERE ##\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=12,\n",
    "    per_device_eval_batch_size=12,\n",
    ")\n",
    "\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"prajjwal1/bert-tiny\", num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cc4b1d",
   "metadata": {},
   "source": [
    "#### Checkpoint 4/4\n",
    "We've defined our training arguments and instantiated our model. Now we need to move the model to the GPU.\n",
    "\n",
    "Use the variable we defined in the first cell to move the model to the GPU if it's available.\n",
    "\n",
    "In the code below that line, we put everything together in Hugging Face's `Trainer` class. Now that the trainer has the model, the arguments, and both datasets, we simply need to call `trainer.train()` to perform our finetuning run.\n",
    "\n",
    "When you're done, evaluate the model's results with `trainer.evaluate()` on the final line of the cell.\n",
    "\n",
    "*Note: You may wonder, 'Why do we not need to move anything other than the model to the GPU in this code? The answer is that the always-ergonomic Hugging Face moves our data over with the `Trainer` class for us. Gotta love that comfortable developer experience!* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3628c83",
   "metadata": {
    "scrolled": true,
    "tags": [
     "cp4"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.14.355, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "[codecarbon INFO @ 17:23:22] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 17:23:22] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 17:23:22] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 17:23:22] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 17:23:22] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon INFO @ 17:23:23] CPU Model on constant consumption mode: Intel(R) Xeon(R) Platinum 8259CL CPU @ 2.50GHz\n",
      "[codecarbon INFO @ 17:23:24] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 17:23:24]   Platform system: Linux-4.14.355-277.647.amzn2.x86_64-x86_64-with-glibc2.35\n",
      "[codecarbon INFO @ 17:23:24]   Python version: 3.10.12\n",
      "[codecarbon INFO @ 17:23:24]   CodeCarbon version: 2.3.1\n",
      "[codecarbon INFO @ 17:23:24]   Available RAM : 30.948 GB\n",
      "[codecarbon INFO @ 17:23:24]   CPU count: 8\n",
      "[codecarbon INFO @ 17:23:24]   CPU model: Intel(R) Xeon(R) Platinum 8259CL CPU @ 2.50GHz\n",
      "[codecarbon INFO @ 17:23:24]   GPU count: 1\n",
      "[codecarbon INFO @ 17:23:24]   GPU model: 1 x Tesla T4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='315' max='315' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [315/315 00:07, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 17:23:35] Energy consumed for RAM : 0.000025 kWh. RAM Power : 11.605434894561768 W\n",
      "[codecarbon INFO @ 17:23:35] Energy consumed for all GPUs : 0.000133 kWh. Total GPU Power : 61.35680445157193 W\n",
      "[codecarbon INFO @ 17:23:35] Energy consumed for all CPUs : 0.000228 kWh. Total CPU Power : 105.0 W\n",
      "[codecarbon INFO @ 17:23:35] 0.000387 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='105' max='105' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [105/105 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.5994669198989868,\n",
       " 'eval_runtime': 1.1814,\n",
       " 'eval_samples_per_second': 1058.068,\n",
       " 'eval_steps_per_second': 88.878,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## YOUR SOLUTION HERE ##\n",
    "model.to(device)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_training_set,\n",
    "    eval_dataset=tokenized_test_set\n",
    ")\n",
    "\n",
    "## YOUR SOLUTION HERE ##\n",
    "trainer.train()\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa3b5e0",
   "metadata": {},
   "source": [
    "Remember our evaluation loss from the base BERT model? So long as you ran the random seed code in that exercise, your loss value in that initial evaluation should've been 0.7226571440696716. The `eval_loss` above is quite the improvement, right?\n",
    "\n",
    "Finally, we'll run the line `model.save_pretrained(\"./our_finetuned_model\")`. This will save the model so we can use it on our own, which we'll go over in greater depth in the next exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3672d0f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./our_finetuned_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
